\section{Introduction}
Learning from Demonstrations (LfD) \cite{DBLP:conf/nips/Schaal96, argall2009survey} has demonstrated promising results in a variety of robotics applications including 
personal robotics \cite{DBLP:conf/icra/BergMDHWFGA10}, autonomous helicopter control \cite{DBLP:conf/iros/AbbeelDNT08}, and control of humanoid robots \cite{DBLP:journals/ras/NakanishiMECSK04}.
In a typical LfD setting, a robot is provided a set of \emph{demonstrations} which are ordered sequences of state-action pairs. 
From these demonstrations, the goal is to learn a \emph{policy} a function of the current state which returns an action.

However, set of all state-actions pairs is usually infinite.
As demonstrations are often human-initiated, and thus, relatively small in number, learning a policy over an infite dimensional state-action space is infeasible.
A recent approach is to parameterize the action space with a finite generating set of action \emph{primitives} \cite{niekum2012learning, abdo2013learning, willsky2009sharing, fox2011joint}.
In the literature, this approach has been refered to as motion primitive learning, sparse coding, and dictionary learning.
LfD can learn a policy over the set of primitives, and then \emph{generalize} this policy by encoding new observations in terms of already learned primitives.
There are many variants of this approach (see Survey \cite{argall2009survey}), and there is also strong connection to recent results in Reinforcement Learning using Policy Gradients \cite{peters2006policy} which find optimal policies by applying a Stochastic Gradient-like method to find optimal parameterized policies.

In this work, we apply these recent results in LfD to surgical robotics.
Increasingly, the surgical robotics community has studied automating surgical subtasks \cite{moustris2011evolution, kehoe2014autonomous, van2010superhuman, mahlerlearning}.
This focus on autonomous execution marks a transition for surgical robots, such as the Da Vinci and the Raven, which originally designed for tele-operation.
There is a wealth of knowledge in the surgical community on precise and accurate tele-operation of these platforms, and expert surgeons are often very skilled and familiar with the tools.
Our goal is to build a learning framework that can learn primitives from these expert demonstrations, and help make autonomous execution of surgical subtasks more robust.
In fact, the seminal work in surgical primitive learning (also called Surgemes) \cite{lin2006towards} was not initially motivated with autonomous execution.
Lin et al. proposed a Bayesian classification framework to model surgical suturing in terms of discrete actions; with the goal of evaluating surgical skill based on expert demonstrations.

The results of Lin et al. introduce some of key domain-specific challenges in surgical primitive learning: (1) demonstration trajectories were heavily pre-processed before learning with de-noising and dimensionality reduction, (2) there was significant gap between expert demonstrations and non-expert demonstrations, and (3) a majority of segmentation errors happened at transition points.
Building on these results and taking these challenges to heart, in this work, we approach the problem of surigical primitive learning from a perspective of robust execution.
\textbf{What are our contributions?}


